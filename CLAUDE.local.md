# Project: SCOUT - Statistical Client Observation & Unified Tracking
Type: data-pipeline
Purpose: Automated GA4 anomaly detection system with ML insights for 50+ STM clients
Stack: BigQuery, Python, Cloud Functions, Next.js, SendGrid, Teams
Status: Implementation Active
Created: 2025-09-25

## Place-Making Specification
**Physical Metaphor**: Scout Tower / Reconnaissance Post
- Elevated observation point with 360-degree visibility
- First line of detection for approaching issues

**Signature Element**: Scout badge (three-tier alert gradient: green/yellow/red)
- Appears in every notification, dashboard, and report
- Visual consistency across all SCOUT touchpoints

**Entry Feeling**: "Protected and informed"
- Immediate confidence that SCOUT is watching
- Sense that nothing slips through

**Transformation**: Data Analyst ‚Üí Pattern Hunter
- From reactive responder to proactive scout
- From manual checker to intelligence officer

**Anti-Atmosphere**: This place is NOT:
- A surveillance state (helpful, not intrusive)
- An alarm factory (intelligent, not noisy)
- A GA4 replacement (augmentation only)

## Atmosphere Requirements
- [AR1] Alert gradient visible in every communication
- [AR2] Response time feels instant for critical anomalies
- [AR3] Error messages guide toward resolution
- [AR4] Success triggers "crisis averted" satisfaction
- [AR5] Beneficial friction: Confirmation before alert dismissal

## ‚ö†Ô∏è CRITICAL: Always Check Dependencies First
**YOU MUST** read STATE.md before starting ANY work to see what's ready to implement.

## üéØ Current Sprint Focus
Week 1-2: Foundation pipeline - Schema discovery and basic anomaly detection
Week 3-4: Intelligence layer - Pattern recognition and root cause analysis
Week 5-6: Advanced features - Segments and predictions

## üìö CRITICAL: Framework Documentation Reference

### TanStack Documentation (MUST REFERENCE BEFORE CODING)
**Location**: `C:\Users\Charles Blain\CascadeProjects\docs\tanstack docs\`
- **TanStack Router**: Routing patterns, nested routes, type-safe navigation
- **TanStack Query**: Data fetching, caching, mutations, optimistic updates
- **TanStack Table**: Column definitions, filtering, sorting, pagination
- **TanStack Form**: Form validation, field management, submission handling

**Rule**: ALWAYS check TanStack docs for patterns before implementing UI features.
Never deviate from documented patterns.

## üß™ Browser Validation Requirements (WEB UI MANDATORY)

All TanStack UI features require Chrome DevTools MCP validation.

### AI-Agent Compatible Validation (Required)
These tools return **text output** and work with all AI agents:
- `take_snapshot` ‚Üí HTML text with element UIDs
- `fill_form`, `click`, `wait_for` ‚Üí Interaction commands
- `list_console_messages` ‚Üí Text console output
- `list_network_requests` ‚Üí JSON response data
- `evaluate_script` ‚Üí Returns primitives/JSON

### Human Documentation (Optional)
- `take_screenshot` ‚Üí PNG images (for human review only)
- `performance_start_trace` ‚Üí Performance data (AI can analyze results)

**All validation can be done without image processing.**

### ConfigTable Validation:
```markdown
BrowserTest: {
  Setup:
    - new_page ‚Üí navigate to http://localhost:5179/configuration
    - take_snapshot ‚Üí verify ConfigTable loaded

  AddProperty:
    - click uid="add-property-button"
    - take_snapshot ‚Üí verify modal open
    - fill uid="property-id-input" value="123456789"
    - fill uid="dataset-id-input" value="analytics_123456789"
    - fill uid="client-name-input" value="Test Client"
    - fill uid="domain-input" value="example.com"
    - click uid="save-button"
    - wait_for "Property saved successfully"
    - list_console_messages ‚Üí expect: no errors
    - take_screenshot ‚Üí save as config-add-success.png

  Assertions:
    - Property appears in table
    - Dashboard cards update (+1 configured)
    - Data persists to Cloud Storage
}
```

### Results Dashboard Validation:
```markdown
BrowserTest: {
  Setup:
    - new_page ‚Üí navigate to http://localhost:5179/results
    - take_snapshot ‚Üí verify Results page loaded
    - list_network_requests resourceTypes=["fetch"]

  FilteringFlow:
    - take_snapshot ‚Üí get filter dropdown UIDs
    - click uid="property-filter-dropdown"
    - click uid="property-option-249571600"
    - wait_for filtered results
    - click uid="severity-filter-dropdown"
    - click uid="severity-critical"
    - take_screenshot ‚Üí filtered-critical.png
    - list_console_messages ‚Üí expect: no errors

  SegmentFilter:
    - click uid="segment-type-dropdown"
    - click uid="segment-device"
    - wait_for segment badge display
    - take_screenshot ‚Üí segment-device-filter.png

  Performance:
    - emulate_network "Fast 3G"
    - performance_start_trace autoStop=true, reload=true
    - performance_stop_trace
    - performance_analyze_insight "LCPBreakdown"
    - expect: LCP < 3000ms
}
```

### Navigation Flow Validation:
```markdown
BrowserTest: {
  FullUserJourney:
    - new_page ‚Üí navigate to http://localhost:5179
    - take_screenshot ‚Üí dashboard-home.png
    - click uid="config-nav-link"
    - wait_for "Property Configuration"
    - click uid="results-nav-link"
    - wait_for "Anomaly Results"
    - click uid="home-nav-link"
    - list_console_messages ‚Üí expect: no errors
    - get_network_request url="/api/config/properties"
    - assert: status 200
}
```

## üèóÔ∏è Architecture Decision (2025-10-01)

### System Components
1. **TanStack React UI** (Configuration & Monitoring)
   - Configure which properties to monitor
   - View anomaly detection results
   - Manage client-specific settings
   - Deploy to Vercel/Netlify

2. **Cloud Run Jobs** (Processing Pipeline)
   - Containerized Python anomaly detection scripts
   - Triggered by Cloud Scheduler (6 AM ET daily)
   - Reads config from Cloud Storage
   - Writes results to Cloud Storage

3. **Google Cloud Storage** (Data Bridge)
   - `gs://scout-config/properties.json` - UI writes configuration
   - `gs://scout-results/anomalies.json` - Jobs write results
   - `gs://scout-results/alerts/` - Historical alert archive

### Implementation Phases
**Phase 1**: UI Configuration (Current)
- Update ConfigTable to use Cloud Storage API
- Deploy frontend to Vercel
- Test configuration workflow

**Phase 2**: Cloud Run Jobs
- Containerize Python scripts with Dockerfile
- Set up Cloud Scheduler
- Configure service account permissions

**Phase 3**: Results Dashboard
- Build anomalies view in TanStack app
- Implement filtering/sorting
- Add alert history

## üìö Project Commands
```bash
# BigQuery Testing
python scripts/test_schema_discovery.py
python scripts/test_anomaly_detection.py

# Local Development
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt

# Cloud Run Jobs Deployment (NEW)
gcloud builds submit --tag gcr.io/stm-mvp/scout-processor
gcloud run jobs create scout-anomaly-detection \
  --image gcr.io/stm-mvp/scout-processor \
  --region us-east1
gcloud scheduler jobs create scout-daily \
  --schedule="0 6 * * *" \
  --time-zone="America/New_York" \
  --uri="https://us-east1-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/stm-mvp/jobs/scout-anomaly-detection:run"

# Cloud Storage Setup
gsutil mb gs://scout-config
gsutil mb gs://scout-results
gsutil cors set cors.json gs://scout-config
```

## üîß Development Workflow (FOLLOW THIS EXACTLY)

### Starting a Session:
1. Check STATE.md for current progress
2. Identify next unblocked feature
3. Review BigQuery costs dashboard
4. Announce implementation plan

### For Each Feature:
1. Write BigQuery schema/query first
2. Test with single client data
3. Build Python processing logic
4. Scale test with 5 clients
5. Deploy to Cloud Function
6. Update STATE.md

## üíª Code Standards (PRODUCTION CRITICAL)
- **SQL Queries**: Always use partitioning and clustering
- **Cost Control**: Estimate query cost before running
- **Error Handling**: Never lose anomaly data
- **Testing**: Use sandbox dataset first
- **Documentation**: Every SQL query needs cost estimate comment

## üì¶ WBS Features Specification

Feature: DataIngestion {
  What:
    - "BigQuery schema discovery for all client properties" [R1]
      ‚Üí provides: schema-registry
    - "Daily extraction of GA4 events data" [R2]
      ‚Üí needs: schema-registry
      ‚Üí provides: raw-data
    - "Handle custom conversion events dynamically" [R3]
      ‚Üí needs: schema-registry
      ‚Üí provides: conversion-tracking
    - "Data validation and quality checks" [R4]
      ‚Üí needs: raw-data
      ‚Üí provides: clean-data

  Boundaries:
    - Process data within 30 minutes for 50 clients
    - Handle up to 10M events per property daily
    - Support 100+ custom event types per client
    - Maintain 99.9% extraction success rate
    - BigQuery costs under $100/month for ingestion

  Success Criteria (Test these):
    - "[R1] working when all custom events discovered without manual config"
      ‚Üí Check: `python scripts/test_schema_discovery.py --clients all`
      ‚Üí Expect: ‚úÖ All 50 clients return complete schema, 0 manual interventions
    - "[R2] validated when daily data available by 6 AM ET consistently"
      ‚Üí Check: `bq query "SELECT COUNT(*) FROM processed.anomalies_daily WHERE DATE(created_at) = CURRENT_DATE() AND TIME(created_at) < '06:00:00'"`
      ‚Üí Expect: ‚úÖ Data present before 6 AM for 29/30 days (96.7% reliability)
    - "[R3] confirmed when zero data loss during extraction"
      ‚Üí Check: `python scripts/validate_extraction_completeness.py --days 7`
      ‚Üí Expect: ‚úÖ 100% record count match between GA4 raw and SCOUT processed
    - "[R4] verified when schema changes handled gracefully"
      ‚Üí Check: Review error logs for schema evolution events
      ‚Üí Expect: ‚úÖ Auto-adaptation logged, no pipeline failures

  Effort: ~16 hours
  Priority: CRITICAL
  Status: [ ] Not Started

  When Done:
    - Enables: All downstream analysis
    - Validates: Data pipeline reliability
    - Atmosphere: "Mission Control has full visibility"
}
Feature: AnomalyDetection {
  What:
    - "Multi-method statistical anomaly detection" [R5]
      ‚Üí needs: clean-data
      ‚Üí provides: raw-anomalies
    - "Business impact scoring for prioritization" [R6]
      ‚Üí needs: raw-anomalies, conversion-tracking
      ‚Üí provides: prioritized-alerts
    - "Cross-client pattern recognition" [R7]
      ‚Üí needs: raw-anomalies
      ‚Üí provides: portfolio-patterns
    - "Configurable sensitivity thresholds" [R8]
      ‚Üí needs: raw-anomalies
      ‚Üí provides: tuned-detection

  Boundaries:
    - Detect anomalies within 5% margin of error
    - Process 50 clients in under 10 minutes
    - Support 20+ metrics per client
    - Maximum 20% false positive rate
    - Use BigQuery ML where possible for cost efficiency

  Success Criteria (Test these):
    - "[R5] working when z-score and IQR methods both operational"
      ‚Üí Check: `python scripts/test_anomaly_detection.py --methods all --clients 5`
      ‚Üí Expect: ‚úÖ Both methods complete without errors, results stored in anomalies_daily
    - "[R6] validated when 90% of real anomalies detected"
      ‚Üí Check: Compare against manually flagged anomalies from first month
      ‚Üí Expect: ‚úÖ Detection rate ‚â• 90% (detect 27+ of 30 known anomalies)
    - "[R7] confirmed when business impact score aligns with AM priorities"
      ‚Üí Check: Survey 10 AMs on top 5 alerts from last week
      ‚Üí Expect: ‚úÖ 80% agreement that highest scored alerts were most important
    - "[R8] verified when portfolio-wide patterns identified weekly"
      ‚Üí Check: `bq query "SELECT pattern_type, COUNT(*) FROM processed.patterns_portfolio WHERE week = CURRENT_WEEK() GROUP BY pattern_type"`
      ‚Üí Expect: ‚úÖ At least 3 distinct pattern types detected across clients

  Effort: ~24 hours
  Priority: CRITICAL
  Status: [ ] Not Started

  When Done:
    - Enables: Intelligent alerting system
    - Validates: Detection accuracy
    - Atmosphere: "Control room spots all threats"
}

Feature: IntelligentAlerting {
  BrowserE2ETest: {
    Scenario: "AM receives alert, investigates in dashboard"

    Steps:
      1. Simulate anomaly detection (backend script)
      2. new_page ‚Üí navigate to http://localhost:5179
      3. take_snapshot ‚Üí verify alert notification
      4. click uid="alert-card-0"
      5. wait_for "Anomaly Details"
      6. take_screenshot ‚Üí anomaly-detail.png
      7. click uid="view-in-results"
      8. wait_for Results page with pre-filtered data
      9. list_console_messages ‚Üí expect: 0 errors
      10. performance_start_trace
      11. click uid="export-csv-button"
      12. performance_stop_trace
      13. expect: Export completes < 2s

    Validation:
      - Full user journey error-free
      - Navigation between pages smooth
      - Data consistency maintained
      - Performance acceptable
  }

  What:
    - "Morning email digest with all anomalies" [R9]
      ‚Üí needs: prioritized-alerts
      ‚Üí provides: email-notifications
    - "Microsoft Teams integration for critical alerts" [R10]
      ‚Üí needs: prioritized-alerts
      ‚Üí provides: instant-alerts
    - "Root cause correlation with external factors" [R11]
      ‚Üí needs: raw-anomalies, portfolio-patterns
      ‚Üí provides: cause-analysis
    - "Predictive alerts for future issues" [R12]
      ‚Üí needs: portfolio-patterns
      ‚Üí provides: predictive-insights

  Boundaries:
    - Email delivery by 7 AM ET daily (99.9% SLA)
    - Teams alerts within 5 minutes of detection
    - 60% accuracy on root cause identification
    - 7-day prediction horizon
    - SendGrid costs under $50/month

  Success Criteria (Test these):
    - "[R9] working when AMs read email first thing Monday"
      ‚Üí Check: SendGrid open rate tracking for Monday 7 AM emails
      ‚Üí Expect: ‚úÖ 85%+ open rate within 2 hours of delivery
    - "[R10] validated when critical alerts addressed within 1 hour"
      ‚Üí Check: Track time between Teams alert and AM response in channel
      ‚Üí Expect: ‚úÖ Average response time < 60 minutes for critical alerts
    - "[R11] confirmed when root cause correctly identified 6/10 times"
      ‚Üí Check: AM feedback survey on 20 anomalies with root cause attribution
      ‚Üí Expect: ‚úÖ 60%+ accuracy (12+ correct identifications out of 20)
    - "[R12] verified when predictions prevent at least 1 issue/month"
      ‚Üí Check: Document prevented issues with before/after screenshots
      ‚Üí Expect: ‚úÖ 1+ documented case per month where prediction enabled early action

  Effort: ~20 hours
  Priority: HIGH
  Status: [ ] Not Started

  When Done:
    - Enables: Proactive client management
    - Validates: Business value delivery
    - Atmosphere: "Mission Control communicates clearly"
}

Feature: SegmentAnalysis {
  What:
    - "Dynamic segment builder interface" [R13]
      ‚Üí needs: schema-registry
      ‚Üí provides: segment-definitions
    - "Segment-level anomaly detection" [R14]
      ‚Üí needs: segment-definitions, clean-data
      ‚Üí provides: segment-anomalies
    - "Cohort behavior tracking" [R15]
      ‚Üí needs: segment-definitions
      ‚Üí provides: cohort-insights
    - "Segment performance predictions" [R16]
      ‚Üí needs: segment-anomalies, predictive-insights
      ‚Üí provides: segment-forecasts

  Boundaries:
    - Support 10 segments per client maximum
    - Process segments within daily batch window
    - Retroactive analysis up to 90 days
    - Segment size minimum 100 users
    - UI built with existing Tremor components

  Success Criteria (Test these):
    - "[R13] working when non-technical user can create segments"
      ‚Üí Tool: new_page http://localhost:5179/configuration
      ‚Üí Tool: take_snapshot ‚Üí verify UI elements
      ‚Üí Tool: click uid="add-segment-button"
      ‚Üí Tool: fill_form with segment criteria
      ‚Üí Tool: click uid="save-segment"
      ‚Üí Tool: wait_for "Segment created"
      ‚Üí Tool: list_console_messages ‚Üí expect: 0 errors
      ‚Üí Tool: take_screenshot ‚Üí segment-created.png
      ‚Üí Metric: Time to completion < 5 minutes
    - "[R14] validated when segment-specific anomalies detected"
      ‚Üí Tool: navigate_page http://localhost:5179/results
      ‚Üí Tool: take_snapshot ‚Üí get segment filter UID
      ‚Üí Tool: click uid="segment-type-dropdown"
      ‚Üí Tool: click uid="segment-device"
      ‚Üí Tool: wait_for segment badge visible
      ‚Üí Tool: take_screenshot ‚Üí segment-anomalies.png
      ‚Üí Tool: evaluate_script ‚Üí return document.querySelectorAll('.anomaly-card').length
      ‚Üí Expect: At least 5 device-specific anomalies displayed
    - "[R15] confirmed when cohort patterns visible in dashboard"
      ‚Üí Check: Visual inspection of dashboard with 2 cohorts active
      ‚Üí Expect: ‚úÖ Clear trend lines, comparison view functional, data accurate
    - "[R16] verified when predictions 70% accurate for segments"
      ‚Üí Check: Compare 7-day predictions against actual for 10 segments
      ‚Üí Expect: ‚úÖ 7+ predictions within 10% margin of actual values

  Effort: ~32 hours
  Priority: MEDIUM
  Status: [ ] Not Started

  When Done:
    - Enables: Granular analysis capability
    - Validates: Advanced insights delivery
    - Atmosphere: "Deep space telemetry online"
}

## üéØ 4-Detector Anomaly System Architecture

**Critical Context**: BigQuery data has 72-hour settling period. All queries MUST use `DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY)` buffer. Daily batch processing at 6 AM ET only (no real-time).

Feature: MultiDetectorAnomalySystem {
  What:
    - "Disaster detection for catastrophic failures" [R17]
      ‚Üí needs: clean-data
      ‚Üí provides: disaster-alerts
    - "Spam detection with quality signal analysis" [R18]
      ‚Üí needs: clean-data, quality-signals
      ‚Üí provides: spam-alerts
    - "Record detection for all-time highs/lows" [R19]
      ‚Üí needs: historical-data-90day
      ‚Üí provides: record-alerts
    - "Trend detection for directional changes" [R20]
      ‚Üí needs: historical-data-180day
      ‚Üí provides: trend-alerts

  Detector Specifications:

    1. DisasterDetector (P0 - Critical):
      - Algorithm: Threshold-based comparison
      - BigQuery Query: `DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY)` to yesterday
      - Comparison: Yesterday vs 3-day average
      - Triggers:
        * sessions < 10 (near-zero traffic)
        * conversions = 0 (tracking failure)
        * 90%+ traffic drop from baseline
      - Dimensions: Overall ONLY (site-wide failures)
      - Output: data/scout_disaster_alerts.json
      - UI Page: /disasters (red banners, "ACT NOW" messaging)

    2. SpamDetector (P1 - High Priority):
      - Algorithm: Z-score (threshold 3.0) + quality signals
      - BigQuery Query: `DATE_SUB(CURRENT_DATE(), INTERVAL 10 DAY)` to yesterday
      - Comparison: Yesterday vs 7-day average
      - Quality Checks:
        * bounce_rate > 85%
        * avg_session_duration < 10s
      - Dimensions: Overall, Geography, Traffic Source
      - Output: data/scout_spam_alerts.json
      - UI Page: /spam (orange warnings with quality metrics)

    3. RecordDetector (P1-P3 - Mixed Priority):
      - Algorithm: Historical max/min comparison
      - BigQuery Query: `DATE_SUB(CURRENT_DATE(), INTERVAL 93 DAY)` to yesterday
      - Comparison: Yesterday vs last 90 days
      - Triggers: New high/low within 90-day window
      - Dimensions: Overall, Landing Pages, Devices, Traffic Source
      - Volume Filter: Min 100 sessions/day (high-traffic segments only)
      - Output: data/scout_record_alerts.json
      - UI Page: /records (trophy icons üèÜ for highs, ‚ö†Ô∏è for lows)

    4. TrendDetector (P2-P3 - Lower Priority):
      - Algorithm: Moving average crossover
      - BigQuery Query: `DATE_SUB(CURRENT_DATE(), INTERVAL 183 DAY)` to yesterday
      - Comparison: Last 30 days avg vs previous 180 days avg
      - Threshold: 15% change (up or down)
      - Dimensions: Overall, Geography, Landing Pages, Devices, Traffic Source
      - Volume Filter: Min 50 sessions/day (meaningful segments)
      - Output: data/scout_trend_alerts.json
      - UI Page: /trends (line charts with trend indicators ‚ÜóÔ∏è‚ÜòÔ∏è)

  Detector-Dimension Matrix (Alert Volume Control):
    Detector       | Overall | Geo  | Pages | Devices | Traffic
    ---------------|---------|------|-------|---------|----------
    Disaster (P0)  | ‚úÖ      | ‚ùå   | ‚ùå    | ‚ùå      | ‚ùå
    Spam (P1)      | ‚úÖ      | ‚úÖ   | ‚ùå    | ‚ùå      | ‚úÖ
    Record (P1-P3) | ‚úÖ      | ‚ùå   | ‚úÖ    | ‚úÖ      | ‚úÖ
    Trend (P2-P3)  | ‚úÖ      | ‚úÖ   | ‚úÖ    | ‚úÖ      | ‚úÖ

  Alert Prioritization:
    P0: Disaster (tracking failure, site down)
    P1: Spam + Record Low (data quality + worst ever)
    P2: Negative Trends (gradual decline)
    P3: Positive Trends + Record High (good news)

  Boundaries:
    - Process 50 clients in under 10 minutes
    - Maximum 12 alerts per client/day
    - BigQuery costs under $150/month for all detectors
    - All data must respect 72-hour settling period
    - Daily batch processing only (6 AM ET)

  Success Criteria (Test these):
    - "[R17] working when disaster alerts catch catastrophic failures"
      ‚Üí Check: `python scripts/scout_disaster_detector.py`
      ‚Üí Expect: ‚úÖ 0-3 disaster alerts per property (site-wide only)
    - "[R18] validated when spam detection identifies bot traffic"
      ‚Üí Check: `python scripts/scout_spam_detector.py`
      ‚Üí Expect: ‚úÖ Quality signals present (bounce_rate, duration)
    - "[R19] confirmed when record detector finds 90-day highs/lows"
      ‚Üí Check: `python scripts/scout_record_detector.py`
      ‚Üí Expect: ‚úÖ Historical context included (90-day max/min)
    - "[R20] verified when trend detector spots 30-day changes"
      ‚Üí Check: `python scripts/scout_trend_detector.py`
      ‚Üí Expect: ‚úÖ Moving average comparison (30-day vs 180-day)

  Effort: ~24 hours
  Priority: HIGH
  Status: [-] In Progress

  When Done:
    - Enables: Comprehensive anomaly coverage
    - Validates: Multi-pattern detection accuracy
    - Atmosphere: "Control tower sees everything"
}

## üö´ Anti-Requirements (DO NOT BUILD - SAVE 400+ HOURS)

### Phase 1 Exclusions
- ‚ùå Real-time streaming analysis (batch is sufficient)
- ‚ùå Client-facing dashboards (internal only)
- ‚ùå Historical backfill beyond 90 days
- ‚ùå Custom ML model training UI
- ‚ùå Competitor monitoring
- ‚ùå Attribution modeling
- ‚ùå Budget pacing alerts
- ‚ùå Custom visualization builder
- ‚ùå GA4 replacement features

### Technical Boundaries
- ‚ùå No support for non-GA4 data sources
- ‚ùå No automated remediation actions
- ‚ùå No natural language querying
- ‚ùå No mobile app
- ‚ùå No white-labeling
- ‚ùå No data export to client systems
- ‚ùå No real-time API for external systems

## üîÑ Implementation Order (Follow Dependencies!)

### Week 1-2: Foundation (Must Complete First)
1. DataIngestion.schema-registry (no dependencies) ‚Üí 4 hours
2. DataIngestion.raw-data (needs: schema-registry) ‚Üí 6 hours
3. DataIngestion.clean-data (needs: raw-data) ‚Üí 4 hours
4. AnomalyDetection.raw-anomalies (needs: clean-data) ‚Üí 8 hours

### Week 3-4: Intelligence Layer (Can Parallelize Some)
5. AnomalyDetection.prioritized-alerts (needs: raw-anomalies) ‚Üí 6 hours
6. AnomalyDetection.portfolio-patterns (parallel with 5) ‚Üí 8 hours
7. IntelligentAlerting.email-notifications (needs: prioritized-alerts) ‚Üí 8 hours
8. IntelligentAlerting.cause-analysis (needs: portfolio-patterns) ‚Üí 6 hours

### Week 5-6: Advanced Features (After Core Stable)
9. IntelligentAlerting.Teams-integration (needs: prioritized-alerts) ‚Üí 4 hours
10. SegmentAnalysis.segment-builder (needs: schema-registry) ‚Üí 12 hours
11. IntelligentAlerting.predictive-alerts (needs: patterns) ‚Üí 8 hours
12. SegmentAnalysis.segment-anomalies (needs: segments) ‚Üí 10 hours

## üìù Session Log
<!-- Claude updates this section during work -->
### Session 1 - Project Initialization
- Created PROJECT-058-AnomalyInsights
- Defined complete WBS with dependencies
- Established Mission Control metaphor
- Ready to begin schema discovery implementation

### Next Session Goals
1. Create BigQuery dataset structure
2. Implement schema discovery function
3. Test with 3 pilot clients
